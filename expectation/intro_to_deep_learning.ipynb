{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MusicalInformatics/miws2024/blob/main/expectation/intro_to_deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-candle",
   "metadata": {},
   "source": [
    "# A Very Quick and Dirty Introduction to Deep Learning and PyTorch\n",
    "\n",
    "\n",
    "\n",
    "Deep learning is a family of machine learning techniques that is the extension of neural networks. In this case, the word *deep* refers to the fact that these neural networks are organized into many **layers**.\n",
    "\n",
    "This short tutorial aims to be a practical introduction to deep learning using [PyTorch](https://pytorch.org/). **Parts of this tutorial are taken from PyTorch's [Introduction to PyTorch](https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-paste",
   "metadata": {},
   "source": [
    "## Quick recap of PyTorch\n",
    "\n",
    "Most computations on neural networks computations are linear algebra operations on **tensors**. Tensors can be thought as a generalization of matrices (the proper mathematical definition of tensors is a [bit more complicated](https://en.wikipedia.org/wiki/Tensor)). \n",
    "\n",
    "A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor, and so on. PyTorch is built around tensors!\n",
    "\n",
    "##### Creating Tensors\n",
    "We'll start with a few basic tensor manipulations. We start with ways of creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-august",
   "metadata": {},
   "source": [
    "It is possible to override the default datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-addition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = torch.ones((5, 3), dtype=torch.int16)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-queensland",
   "metadata": {},
   "source": [
    "We can also initialize tensors with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2)\n",
    "print('A random tensor:')\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print('\\nA different random tensor:')\n",
    "print(r2) # new values\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print('\\nShould match r1:')\n",
    "print(r3) # repeats values of r1 because of re-seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-joint",
   "metadata": {},
   "source": [
    "We can initialize a tensor from a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-recognition",
   "metadata": {},
   "source": [
    "And convert the tensors back to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_tensor = t.numpy()\n",
    "print(f'numpy_tensor type {type(numpy_tensor)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-boards",
   "metadata": {},
   "source": [
    "##### Basic Arithmetic and Elementwise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # additon allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-claim",
   "metadata": {},
   "source": [
    "**Exercise**: The following snippet results in a runtime error. Why is that? (you have to uncomment the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funded-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "\n",
    "# r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-february",
   "metadata": {},
   "source": [
    "Elementwise operations and aggregate operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.rand(2, 2) - 0.5 * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-excellence",
   "metadata": {},
   "source": [
    "##### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-honolulu",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1 = torch.rand(2, 2) - 0.5 * 2 # values between -1 and 1\n",
    "print('A random matrix, r1:')\n",
    "print(r)\n",
    "r2 = torch.rand(2, 2) - 0.5 * 3 # values between -1.5 and 1.5\n",
    "print('A random matrix, r2:')\n",
    "print(r2)\n",
    "\n",
    "print('\\nMatrix Multiplication of r1 and r2')\n",
    "print(torch.matmul(r1, r2))\n",
    "print('\\nDeterminant of r1:')\n",
    "print(torch.det(r1))\n",
    "print('\\nSingular value decomposition of r1:')\n",
    "print(torch.svd(r1))\n",
    "print('\\nPseudo-inverse of r1:')\n",
    "print(torch.pinverse(r1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-illinois",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "Neural networks have its origins in early work that tried to model biological networks of neurons in the brain [(McCulloch and Pits, 1943)](https://homes.luddy.indiana.edu/jbollen/I501F13/readings/mccullochpitts1943.pdf). For this reason, these methods are called neural networks, although the resemblance to real neural cells is just superficial.\n",
    "\n",
    "We can understand artificial neural networks (or simply neural networks) as **complex compositions of simpler functions**. Each node within a network is called a **unit**, and each of these units calculates a weighted sum of the inputs from predecessor nodes and then applies a nonlinear function.\n",
    "\n",
    "Let us consider the following simple case:\n",
    "\n",
    "Let $a_j$ denote the output of unit $j$ can be computed as\n",
    "\n",
    "$$a_j = g_j\\left(\\sum_{i} w_{ij}a_i + b_j\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "* $g_j(\\cdot)$ is a nonlinear **activation function** associated with unit $j$\n",
    "* $w_{ij}$ is the weight attached to the link from unit $i$ to unit $j$\n",
    "* $b_j$ is a scalar bias\n",
    "\n",
    "with this convention, we can write the above equation in vector form as\n",
    "\n",
    "$$a_j = g_j\\left(\\mathbf{w}_j^T\\mathbf{x} + b_j\\right)$$\n",
    "\n",
    "where $\\mathbf{w}_j^T$ is the vector of weights leading into unit $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-connecticut",
   "metadata": {},
   "source": [
    "##### Activation Functions\n",
    "\n",
    "Some of the most common activation functions are\n",
    "\n",
    "* **Sigmoid** function\n",
    "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_numpy(x):\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "    \n",
    "x = np.linspace(-3, 3)\n",
    "\n",
    "sig_numpy = sigmoid_numpy(x)\n",
    "\n",
    "plt.plot(x, sig_numpy)\n",
    "plt.ylim((0, 1))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-auditor",
   "metadata": {},
   "source": [
    "* **ReLU** (rectified linear units)\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_numpy(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "\n",
    "relu = relu_numpy(x)\n",
    "\n",
    "plt.plot(x, relu)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-pottery",
   "metadata": {},
   "source": [
    "* **Hyperbolic tangent**\n",
    "\n",
    "$$\\tanh(x) = \\frac{\\exp(2x) - 1}{\\exp(2x) + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3)\n",
    "tanh = np.tanh(x)\n",
    "plt.plot(x, tanh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-dutch",
   "metadata": {},
   "source": [
    "### Computation Graphs\n",
    "\n",
    "In the following we will consider $\\mathbf{x}$ an input (training or test) example), $\\hat{\\mathbf{y}}$ are the outputs of the network and $\\mathbf{y}$ the *true values* to derive a learning signal\n",
    "\n",
    "* **Input Encoding**: It depends on the problem we want to model. Assume that we have $n$ input nodes\n",
    "    * If we have Boolean inputs, *false* is usually mapped to $0$ and *true* to $1$, although sometimes $-1$ and $1$ are used\n",
    "    * If we have real valued inputs, we can just use the actual values, although it is common to scale the inputs to fit a fixed range, or use a transformation like a log scale if the magnitudes of the different examples vary a lot.\n",
    "    * If we have categorical encodings, we can use a *one-hot* encoding\n",
    "    \n",
    "* **Output Layers and Loss Function**: On the output side of the network, the problem of encoding raw data values into actual values $\\mathbf{y}$ is very similar than the input encoding: We can use a numerical mapping for Boolean outputs, (scaled/transformed) real values for real-valued outputs, one-hot encodings for categorical data. This can be achieved by choosing an appropriate output nonlinearity:\n",
    "    * For Boolean outputs, we can use the sigmoid function (if we are mapping *false* and *true* to 0 and 1, respectively), or tanh (if we are mapping to -1 and 1).\n",
    "    * For categorical problems, we can use a **softmax** layer:\n",
    "    $$\\text{softmax}(\\mathbf{in})_k = \\frac{\\exp(in_k)}{\\sum_{l=1}^{d} \\exp(in_l)}$$\n",
    "    where $\\mathbf{in} = (in_1, \\dots, in_d)$ are the input values.\n",
    "    * For regression problems, it is usual to use the identity function $g(x) = x$\n",
    "    * And many more, depending on the problem (e.g., mixture density layers).\n",
    "    \n",
    "* **Hidden Layers**: We can think of the hidden layers as learning different *representations* for the input $\\mathbf{x}$. In many cases, the $l$-th hidden layer will be given as a function of the previous layers:\n",
    "\n",
    "$$\\mathbf{h}_l(\\mathbf{h}_{l-1}) = g_l(\\mathbf{W}\\mathbf{h}_{l-1} + \\mathbf{b}_l)$$\n",
    "\n",
    "although this form would depend on the particular neural architecture. (the above example would be for a fully connected feed forward neural network). With this notation, we can write the inputs as the $0$-th layer $\\mathbf{h}_0 = \\mathbf{x}$ and the outputs as the $L$-th layer, i.e., $\\hat{\\mathbf{y}} = \\mathbf{h}_L(\\mathbf{h}_{l-1})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac68f5",
   "metadata": {},
   "source": [
    "\n",
    "##### Training the network\n",
    "\n",
    "The **loss function** is a measure of how good the predictions of the network are, i.e., how close do the predictions of the network approximate the expected values $\\mathbf{y}$. We can use this loss function to learn the parameters of the network (the sets of weights and biases) as those which minimize the loss function\n",
    "\n",
    "$$\\hat{\\mathbf{\\theta}} = \\arg \\min_{\\mathbf{\\theta}} \\mathcal{L}(\\mathbf{Y}, \\hat{\\mathbf{Y}})$$\n",
    "\n",
    "###### Regression Problems\n",
    "\n",
    "For regression problems, it is common to use the mean squared error\n",
    "\n",
    "$$\\text{mse}(\\mathbf{Y}, \\hat{\\mathbf{Y}}) = \\frac{1}{N} \\sum_{i}||\\mathbf{y}_i - \\hat{\\mathbf{y}}_i||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb87661",
   "metadata": {},
   "source": [
    "\n",
    "###### Classification Problems\n",
    "\n",
    "The **Cross Entropy Loss** is a common loss function used in classification tasks. It measures the difference between two probability distributions: the true distribution (ground truth labels) and the predicted distribution (model predictions). In other words, it quantifies how well the model’s predicted probabilities align with the actual classes.\n",
    "\n",
    "Mathematically, Cross Entropy Loss for a single data point is given by:\n",
    "\n",
    "\n",
    "$$\\text{Cross Entropy} = -\\sum_{i=1}^{C} y_i \\log(p_i)$$\n",
    "\n",
    "where:\n",
    "- $C$ is the number of classes.\n",
    "- $y_i$ is the actual label (1 if the sample belongs to class $i$, 0 otherwise).\n",
    "- $p_i$ is the predicted probability of the sample being in class $i$.\n",
    "\n",
    "The loss function penalizes incorrect predictions more as the probability for the correct class decreases, encouraging the model to assign higher probabilities to correct classes.\n",
    "\n",
    "#### Why Cross Entropy?\n",
    "\n",
    "If the model confidently predicts the correct class (i.e., assigns a high probability close to 1 for the correct class), the cross-entropy loss will be low. However, if the model predicts a low probability for the correct class, the loss will be high. This loss function is particularly effective for training neural networks to output probabilities that align closely with the ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4463d02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Visualizing Cross Entropy Loss\n",
    "\n",
    "To illustrate Cross Entropy Loss, let’s plot how the loss changes with different predicted probabilities for a correct class label. A lower probability for the correct class leads to a higher loss, whereas a probability closer to 1 leads to a lower loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define probabilities and compute cross entropy loss for a correct class (label = 1)\n",
    "probabilities = np.linspace(0.01, 1, 100)  # Predicted probabilities from 0.01 to 1\n",
    "cross_entropy_loss = -np.log(probabilities)  # Cross-entropy loss for the correct class\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(probabilities, cross_entropy_loss, label=\"Cross Entropy Loss\", color=\"blue\")\n",
    "plt.title(\"Cross Entropy Loss vs. Predicted Probability for Correct Class\")\n",
    "plt.xlabel(\"Predicted Probability for Correct Class\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ce7fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The visualization above shows how Cross Entropy Loss varies with the predicted probability for the correct class. As the probability for the correct class increases (moving right on the x-axis), the loss decreases. When the predicted probability approaches 1, the loss approaches zero, indicating a confident and accurate prediction. Conversely, lower probabilities for the correct class yield higher losses, pushing the model to learn from incorrect or less confident predictions. \n",
    "\n",
    "This behavior encourages the model to assign high probabilities to correct classes, making Cross Entropy Loss a powerful choice for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b573d89",
   "metadata": {},
   "source": [
    "## Training Neural Networks with PyTorch\n",
    "This tutorial will guide you through a simple process of training a neural network on the MNIST dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85912e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # For building neural networks\n",
    "import torch.optim as optim  # For optimization\n",
    "from torchvision import datasets, transforms  # For loading datasets\n",
    "from torch.utils.data import DataLoader  # For batching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1886a",
   "metadata": {},
   "source": [
    "### 1. Prepare Data\n",
    "We'll use the MNIST dataset, a collection of 28x28 grayscale images of handwritten digits (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7672bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: Convert images to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load training and test sets\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders for batching\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0362a",
   "metadata": {},
   "source": [
    "###  2. Define a Neural Network\n",
    "We'll build a simple fully connected neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aea7e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)       # Second hidden layer\n",
    "        self.fc3 = nn.Linear(64, 10)        # Output layer (10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de3801",
   "metadata": {},
   "source": [
    "### 3. Set Up Loss and Optimizer\n",
    "We'll use Cross-Entropy Loss and the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cc1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86583b1b",
   "metadata": {},
   "source": [
    "### 4. Train the Model\n",
    "We'll train the model over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # Number of times to iterate through the dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4a64c",
   "metadata": {},
   "source": [
    "### 5. Test the Model\n",
    "Now we will evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get class with highest score\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a412a65",
   "metadata": {},
   "source": [
    "\n",
    "Save the trained model to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560871d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simple_nn.pth')\n",
    "print(\"Model saved as 'simple_nn.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
