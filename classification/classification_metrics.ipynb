{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1d2f5b",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "In this notebook, we will explore various classification metrics using simple examples. We will cover the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. Precision, Recall, F1-score\n",
    "4. Macro vs. Micro F1-score\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with some definitions:\n",
    "\n",
    "- **True Positives (TP)**: Instances where the model correctly predicts the positive class. That is, cases where both the actual class and the predicted class are positive.\n",
    "\n",
    "- **True Negatives (TN)**: Instances where the model correctly predicts the negative class. That is, cases where both the actual class and the predicted class are negative.\n",
    "\n",
    "Similarly, we have:\n",
    "\n",
    "- **False Positives (FP)**: Instances where the model incorrectly predicts the positive class. These are cases where the actual class is negative, but the model predicts it as positive.\n",
    "\n",
    "- **False Negatives (FN)**: Instances where the model incorrectly predicts the negative class. These are cases where the actual class is positive, but the model predicts it as negative.\n",
    "\n",
    "These concepts are crucial because they help us understand not just how often the model is correct, but the types of errors it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e96b0",
   "metadata": {},
   "source": [
    "# 1. Accuracy\n",
    "\n",
    "**Accuracy** is one of the simplest and most intuitive evaluation metrics for classification models. It is defined as the proportion of correct predictions made by the model out of all predictions made.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Let's consider a simple binary classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n",
    "\n",
    "# Predicted labels\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "# Calculate accuracy manually\n",
    "correct_predictions = np.sum(y_true == y_pred)\n",
    "total_predictions = len(y_true)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy (manual calculation): {accuracy:.2f}\")\n",
    "\n",
    "# Calculate accuracy using sklearn\n",
    "accuracy_sk = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy (sklearn): {accuracy_sk:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65a6ae",
   "metadata": {},
   "source": [
    "## 2. Confusion Matrix\n",
    "\n",
    "The **Confusion Matrix** is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows easy identification of confusion between classes.\n",
    "\n",
    "## Confusion Matrix for Binary Classification\n",
    "\n",
    "The confusion matrix for binary classification is a 2x2 matrix:\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "\n",
    "Continuing with the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display confusion matrix in a DataFrame\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=['Actual Negative', 'Actual Positive'],\n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a4353",
   "metadata": {},
   "source": [
    "## 3. Precision, Recall, F1-score\n",
    "\n",
    "These are more detailed metrics that consider the types of errors the model makes.\n",
    "\n",
    "### Precision\n",
    "\n",
    "**Precision** is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "### Recall\n",
    "\n",
    "**Recall** (also known as Sensitivity or True Positive Rate) is the ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### F1-score\n",
    "\n",
    "**F1-score** is the weighted average of Precision and Recall.\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87bcd7",
   "metadata": {},
   "source": [
    "## 4. Macro vs. Micro F1-score\n",
    "\n",
    "In multiclass classification, the F1-score can be averaged in different ways. The two most common methods are **macro** and **micro** averaging.\n",
    "\n",
    "### Macro Averaging\n",
    "\n",
    "**Macro F1-score** calculates the F1-score independently for each class and then takes the average.\n",
    "\n",
    "$$\n",
    "\\text{Macro F1-score} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{F1-score}_i\n",
    "$$\n",
    "\n",
    "Where $C$ is the number of classes. For imbalanced datasets, it is better to use the macro F1-score!\n",
    "\n",
    "### Micro Averaging\n",
    "\n",
    "**Micro F1-score** calculates metrics globally by counting the total true positives, false negatives, and false positives.\n",
    "\n",
    "$$\n",
    "\\text{Micro F1-score} = \\frac{2 \\times \\text{Precision}_{\\text{micro}} \\times \\text{Recall}_{\\text{micro}}}{\\text{Precision}_{\\text{micro}} + \\text{Recall}_{\\text{micro}}}\n",
    "$$\n",
    "\n",
    "Let's create a simple multiclass example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31662647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "y_true_multi = np.array([0, 1, 2, 0, 1, 2])\n",
    "\n",
    "# Predicted labels\n",
    "y_pred_multi = np.array([0, 2, 1, 0, 0, 1])\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c15365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro F1-score\n",
    "f1_macro = f1_score(y_true_multi, y_pred_multi, average='macro')\n",
    "print(f\"Macro F1-score: {f1_macro:.2f}\")\n",
    "\n",
    "# Micro F1-score\n",
    "f1_micro = f1_score(y_true_multi, y_pred_multi, average='micro')\n",
    "print(f\"Micro F1-score: {f1_micro:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b6ec6",
   "metadata": {},
   "source": [
    "- **Macro F1-score** treats all classes equally by averaging the F1-scores of each class.\n",
    "- **Micro F1-score** gives equal weight to each instance by considering the total true positives, false negatives, and false positives.\n",
    "\n",
    "In this example, the micro F1-score is higher than the macro F1-score because the correct predictions are concentrated in one class (class 0), which increases the overall count of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa445cc9",
   "metadata": {},
   "source": [
    "## 6. Interactive Multi-Class Classification Example\n",
    "\n",
    "In this section, we'll create an interactive example to explore how micro and macro F1-scores change in a multi-class classification scenario. \n",
    "\n",
    "First, let's set up the interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 3\n",
    "classes = [0, 1, 2]\n",
    "\n",
    "# Create sliders for each cell in the confusion matrix\n",
    "sliders = {}\n",
    "for i in classes:\n",
    "    for j in classes:\n",
    "        sliders[f\"C{i}{j}\"] = widgets.IntSlider(value=0, min=0, max=20, description=f\"C{i}{j}\")\n",
    "\n",
    "# Arrange the sliders in a grid\n",
    "conf_matrix_ui = widgets.GridBox(\n",
    "    children=[sliders[f\"C{i}{j}\"] for i in classes for j in classes],\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        grid_template_columns='repeat(3, 200px)',\n",
    "        grid_template_rows='repeat(3, 60px)',\n",
    "        grid_gap='17px'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize confusion matrix with some values\n",
    "sliders[\"C00\"].value = 5  # True class 0 predicted as class 0\n",
    "sliders[\"C11\"].value = 4  # True class 1 predicted as class 1\n",
    "sliders[\"C22\"].value = 6  # True class 2 predicted as class 2\n",
    "sliders[\"C01\"].value = 2  # True class 0 predicted as class 1\n",
    "sliders[\"C12\"].value = 3  # True class 1 predicted as class 2\n",
    "sliders[\"C20\"].value = 1  # True class 2 predicted as class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad3b1b",
   "metadata": {},
   "source": [
    "Now, let's define a function that will update the metrics based on the confusion matrix values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(**kwargs):\n",
    "    # Build the confusion matrix from the sliders\n",
    "    cm = np.array([\n",
    "        [kwargs[f\"C00\"], kwargs[f\"C01\"], kwargs[f\"C02\"]],\n",
    "        [kwargs[f\"C10\"], kwargs[f\"C11\"], kwargs[f\"C12\"]],\n",
    "        [kwargs[f\"C20\"], kwargs[f\"C21\"], kwargs[f\"C22\"]],\n",
    "    ])\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"Actual {i}\" for i in classes], columns=[f\"Predicted {i}\" for i in classes])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    display(cm_df)\n",
    "    \n",
    "    # Flatten the confusion matrix to get true labels and predicted labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in classes:\n",
    "        for j in classes:\n",
    "            count = cm[i][j]\n",
    "            y_true.extend([i]*count)\n",
    "            y_pred.extend([j]*count)\n",
    "    \n",
    "    if len(y_true) == 0:\n",
    "        print(\"No samples to evaluate.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"Precision (Macro): {precision_macro:.2f}\")\n",
    "    print(f\"Recall (Macro): {recall_macro:.2f}\")\n",
    "    print(f\"F1-score (Macro): {f1_macro:.2f}\")\n",
    "    \n",
    "    print(\"\\nMicro-Averaged Metrics:\")\n",
    "    print(f\"Precision (Micro): {precision_micro:.2f}\")\n",
    "    print(f\"Recall (Micro): {recall_micro:.2f}\")\n",
    "    print(f\"F1-score (Micro): {f1_micro:.2f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f892e",
   "metadata": {},
   "source": [
    "Now, we can set up the interactive display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = widgets.interactive_output(update_metrics, sliders)\n",
    "display(conf_matrix_ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e89c5",
   "metadata": {},
   "source": [
    "- Each slider represents the count of instances where a true class is predicted as a certain class.\n",
    "- For example, `C01` represents the number of instances where the true class is 0 but predicted as 1.\n",
    "- Adjust the sliders to change the confusion matrix and observe how the macro and micro F1-scores change.\n",
    "\n",
    "- **Macro-Averaged Metrics**: Calculate the metric independently for each class and then take the average (treat all classes equally).\n",
    "- **Micro-Averaged Metrics**: Aggregate the contributions of all classes to compute the average metric (treat all instances equally).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93b2f3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored various classification metrics using simple examples. Understanding these metrics is crucial for evaluating and improving classification models.\n",
    "\n",
    "- **Accuracy** is simple but can be misleading in imbalanced datasets.\n",
    "- **Confusion Matrix** provides detailed insight into the types of errors made.\n",
    "- **Precision, Recall, F1-score** offer a balance between precision and recall.\n",
    "- **Macro vs. Micro F1-score** are important when dealing with multiclass classification and imbalanced datasets.\n",
    "- **True Positives and True Negatives** are fundamental concepts that help us understand the correctness of our model's predictions in detail.\n",
    "- **Interactive Multi-Class Example** allows us to visualize how changes in the confusion matrix affect macro and micro-averaged metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
