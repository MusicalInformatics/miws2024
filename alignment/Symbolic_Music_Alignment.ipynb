{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f474445",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MusicalInformatics/miws2024/blob/main/alignment/Symbolic_Music_Alignment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d55e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install python packages\n",
    "    # Issues on Colab with newer versions of MIDO\n",
    "    ! pip install mido==1.2.10\n",
    "    ! pip install partitura\n",
    "    ! pip install fastdtw\n",
    "\n",
    "    # To be able to access helper modules in the repo for this tutorial\n",
    "    # (not necessary if the jupyter notebook is run locally instead of google colab)\n",
    "    !git clone https://github.com/neosatrapahereje/music_alignment_tutorial\n",
    "    !git clone --recurse-submodules https://github.com/MusicalInformatics/miws2024.git\n",
    "    import sys\n",
    "    sys.path.insert(0, \"./miws2024/alignment/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c04b411e",
   "metadata": {},
   "source": [
    "# Symbolic Music Alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64631779",
   "metadata": {},
   "source": [
    "Automatic Music Alignment refers to the task of linking or matching two musical signals of the same musical work. This can be, e.g., matching *different performances* of the same piece, or matching the performance of a piece with its musical score.\n",
    "\n",
    "<img src=\"figures/alignment_figure.gif\" alt=\"alignment_figure\" width=\"600\"/>\n",
    "\n",
    "The following figure shows a common music alignment pipeline:\n",
    "\n",
    "<img src=\"figures/alignment_pipeline.png\" alt=\"alignment_pipeline\" width=\"600\"/>\n",
    "\n",
    "In this notebook we are going to explore these components in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c191c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing some stuff\n",
    "import os \n",
    "# import glob\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import partitura as pt\n",
    "\n",
    "from alignment import fast_dynamic_time_warping, greedy_note_alignment\n",
    "\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "if IN_COLAB:\n",
    "    V4X22_DATASET_DIR = \"./miws2024/vienna4x22\"\n",
    "else:\n",
    "    # Path to the Vienna 4x22 dataset\n",
    "    # Get the absolute path of the current notebook\n",
    "    notebook_path = os.path.abspath(\".\")\n",
    "    V4X22_DATASET_DIR = os.path.join(os.path.dirname(notebook_path), \"vienna4x22\")\n",
    "\n",
    "MUSICXML_DIR = os.path.join(V4X22_DATASET_DIR, \"musicxml\")\n",
    "MIDI_DIR = os.path.join(V4X22_DATASET_DIR, \"midi\")\n",
    "MATCH_DIR = os.path.join(V4X22_DATASET_DIR, \"match\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb6fe7b7",
   "metadata": {},
   "source": [
    "## 1. Music Representation\n",
    "\n",
    "Music representations, since this is a tutorial on symbolic music processing, we will focus on symbolic music representations, such that can be stored in formats such as MIDI, MusicXML or MEI, and that can be generated by editors like MuseScore, Finale, etc.\n",
    "\n",
    "### 1.1 Audio vs. Symbolic Alignment\n",
    "\n",
    "* In **Audio-based alignment**, the alignment itself typically refers to  of *timestamps* (in absolute time in seconds) in one audio recording of a musical work to the corresponding *timestamp* in another recording. (In audio recordings, identifying individual notes is not a trivial task)\n",
    "\n",
    "\n",
    "* In **Symbolic-based alignment**, we can have two types of alignment:\n",
    "    * **Time-wise alignments**: similar to audio-based alignment, we can map timestamps (in symbolic time units like musical beats or MIDI ticks) from one version of the work to another (e.g., a MIDI performance to a score in MusicXML/MEI/Humdrum format).\n",
    "\n",
    "    * **Note-wise alignment**: We can map individual symbolic music elements (most commonly notes) from one version to another. This is very useful for modeling expressive performance.\n",
    "\n",
    "\n",
    "### 1.2 Types of music alignment\n",
    "\n",
    "We can categorize musical alignment in two main dimensions: (representation) modality and time.\n",
    "\n",
    "#### 1.2.1 Representation modality\n",
    "\n",
    "\n",
    "* **Audio-to-audio alignment**: Alignment of two (audio) recordings. This is probably the most studied type of alignment in the MIR literature.\n",
    "\n",
    "* **Symbolic-to-audio alignment**: Alignment of symbolically encoded musical events with timestamps in an audio recording.\n",
    "\n",
    "* **Symbolic-to-symbolic alignment**: Alignment of symbolically encoded musical events in two recordings/documents of the same work.\n",
    "\n",
    "* **Image-to-audio alignment**: Alignment of spatial positions of (images) of sheet music with timestamps in an audio recording.\n",
    "\n",
    "* **Lyrics-to-audio alignment**: Alignment of lyrics (text) with timestamps in an audio recording.\n",
    "\n",
    "#### 1.2.2 Time\n",
    "\n",
    "* **Offline**: Alignment of two *recordings/documents* (i.e., audio recordings, MIDI performances, MusicXML scores, etc.). These recordings/documents can be in any of the modalities described above, the important thing being that the music is occurring in real-time.\n",
    "\n",
    "* **Online**: Alignment of a live (i.e., real time) performance to the music encoded in a target document (e.g., a pre-annotated audio recording, a symbolic score, etc.). The problem of real time online alignment is known in the MIR literature a **score following**, and can be useful in live interactive settings, such as automatic accompaniment systems\n",
    "\n",
    "In this tutorial we are going to focus on the case of offline alignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "floating-madison",
   "metadata": {},
   "source": [
    "### 1.3 Representing Alignments\n",
    "\n",
    "#### 1.3.1 The Match file format\n",
    "\n",
    "* Format that extends a MIDI human performance with note-, beat-, and downbeat-level alignments to a corresponding musical score. \n",
    "\n",
    "* Enables advanced analyses of the performance that are relevant for various tasks, such as\n",
    "\n",
    "    * expressive performance modeling, \n",
    "    * score following, \n",
    "    * music transcription\n",
    "    * performer classification, etc. \n",
    "\n",
    "* The match file includes a set of score-related descriptors that makes it usable also as a bare-bones score representation. \n",
    "\n",
    "<img src=\"figures/matchfile_schema.png\" alt=\"matchfile_schema\" width=\"600\"/>\n",
    "\n",
    "The documentation of the matchfile format can be found [here](https://cpjku.github.io/matchfile/).\n",
    "\n",
    "#### 1.3.2 Loading Alignments\n",
    "\n",
    "An important use case of partitura is to handle symbolic alignment information\n",
    "\n",
    "**Note that partitura itself does not contain methods for alignment**\n",
    "\n",
    "Partitura supports 2 formats for encoding score-to-performance alignments\n",
    "\n",
    "* Our match file format\n",
    "    * Datasets including match files: Vienna4x22, ASAP, Batik (soon!)\n",
    "* The format introduced by [Nakamura et al. (2017).](https://eita-nakamura.github.io/articles/EN_etal_ErrorDetectionAndRealignment_ISMIR2017.pdf)\n",
    "\n",
    "Let's load an alignment!\n",
    "\n",
    "We have two common use cases\n",
    "\n",
    "* We have both the match file and the symbolic score file (e.g., MusicXML or MEI)\n",
    "* We have only the match file (only works for our format!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "crucial-virus",
   "metadata": {},
   "source": [
    "##### 1.3.2.1 Loading an alignment if we only have a match file\n",
    "\n",
    "A useful property of match files is that they include information about the **score and the performance**. Therefore, it is possible to create both a  `Part` and a `PerformedPart` directly from a match file.\n",
    "\n",
    "* Match files contain all information included in performances in MIDI files, i.e., a MIDI file could be reconstructed from a match file.\n",
    "\n",
    "* Match files include all information information about pitch spelling and score position and duration of the notes in the score, as well as time and key signature information, and can encode some note-level markings, like accents. Nevertheless, it is important to note that the score information included in a match file is not necessarily complete. For example, match files do not generally include dynamics or tempo markings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6260fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing some stuff\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import partitura as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "rolled-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, \"Chopin_op10_no3_p01.match\")\n",
    "\n",
    "# loading a match file\n",
    "performance, alignment, score = pt.load_match(match_fn, create_score=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wooden-looking",
   "metadata": {},
   "source": [
    "##### 1.3.2.2 Loading an alignment if we have both score and match files\n",
    "\n",
    "In many cases, however, we have access to both the score and match files. Using the original score file has a few advantages:\n",
    "\n",
    "* It ensures that the score information is correct. Generating a `Part` from a match file involves inferring information for non-note elements (e.g., start and end time of the measures, voice information, clefs, staves, etc.).\n",
    "* If we want to load several performances of the same piece, we can load the score only once!\n",
    "\n",
    "This should be the preferred way to get alignment information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "latest-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, \"Chopin_op10_no3_p01.match\")\n",
    "\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, \"Chopin_op10_no3.musicxml\")\n",
    "\n",
    "# Load the score into a `Score` object\n",
    "score = pt.load_musicxml(score_fn)\n",
    "\n",
    "# loading a match file\n",
    "performance, alignment = pt.load_match(match_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pending-college",
   "metadata": {},
   "source": [
    "Score-to-performance alignments are represented by lists of dictionaries, which contain the following keys:\n",
    "\n",
    "* `label`\n",
    "\n",
    "    * `'match'`: there is a performed note corresponding to a score note\n",
    "    * `'insertion'`: the performed note does not correspond to any note in the score\n",
    "    * `'deletion'`: there is no performed note corresponding to a note in the score\n",
    "    * `'ornament'`: the performed note corresponds to the performance of an ornament (e.g., a trill). These notes are matched to the main note in the score. Not all alignments (in the datasets that we have) include ornamnets! Otherwise, ornaments are just treated as insertions.\n",
    "* `score_id`: id of the note in the score (in the `Part` object) (only relevant for matches, deletions and ornaments)\n",
    "* `performance_id`: Id of the note in the performance (in the `PerformedPart`) (only relevant for matches, insertions and ornaments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "exact-decrease",
   "metadata": {},
   "source": [
    "#### 1.3.3 Getting information from the alignments\n",
    "\n",
    "Partitura includes a few methods for getting information from the alignments.\n",
    "\n",
    "Let's start by getting the subset of score notes that have a corresponding performed note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "published-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note array of the score\n",
    "snote_array = score.note_array()\n",
    "# note array of the performance\n",
    "pnote_array = performance.note_array()\n",
    "# indices of the notes that have been matched\n",
    "matched_note_idxs = pt.musicanalysis.performance_codec.get_matched_notes(\n",
    "    spart_note_array=snote_array,\n",
    "    ppart_note_array=pnote_array,\n",
    "    alignment=alignment,\n",
    ")\n",
    "\n",
    "# note array of the matched score notes\n",
    "matched_snote_array = snote_array[matched_note_idxs[:, 0]]\n",
    "# note array of the matched performed notes\n",
    "matched_pnote_array = pnote_array[matched_note_idxs[:, 1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b61b323c",
   "metadata": {},
   "source": [
    "## 2. Feature Representations\n",
    "\n",
    "To make musical data comparable for alignment algorithms, the first step is to extract features that capture relevant aspects while suppressing irrelevant details.\n",
    "\n",
    "Let's make a quick mathematical parenthesis. For algorithmic purposes, it is convenient to represent the music captured by whichever music representation that we working with as *sequences of features*. \n",
    "\n",
    "Let us consider two sequences $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots \\mathbf{x}_N\\}$ and $\\mathbf{Y} = \\{\\mathbf{y}_1, \\dots, \\mathbf{y}_M\\}$ for which we want to find an aligment.\n",
    "\n",
    "* This sequences could be discrete signals, feature sequences, sequences of characters, etc.\n",
    "\n",
    "* The elements $\\mathbf{x}_n$, $\\mathbf{y}_m$ belong to the same **feature space** $\\mathcal{F}$. For the purposes of this tutorial, let us consider these elements as $K$-dimensional real-vectors, i.e., $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^K$ although they can other kind of objects (e.g., characters in an alphabet).\n",
    "\n",
    "An important aspect of this feature space is that it allows us to use *quantitive measures* of how similar the elements of sequence $\\mathbf{X}$ are to the elements in sequence $\\mathbf{Y}$. We will come back to this point in a moment.\n",
    "\n",
    "In this tutorial we are going to focus on 2 common features representations:\n",
    "\n",
    "1. Piano Rolls\n",
    "2. Pitch Class Distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36cc0f43",
   "metadata": {},
   "source": [
    "### 2.1 Piano Rolls\n",
    "\n",
    "A piano roll is a 2D representation of (MIDI) pitch and time. We can extract piano rolls from symbolic music files with Partitura!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63cf2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import generate_example_sequences, plot_alignment\n",
    "from alignment import greedy_note_alignment\n",
    "\n",
    "from typing import List\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "individual-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a score and a performance of the score\n",
    "\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, \"Chopin_op10_no3.musicxml\")\n",
    "performance_fn = os.path.join(MIDI_DIR, \"Chopin_op10_no3_p01.mid\")\n",
    "\n",
    "score = pt.load_score(score_fn)\n",
    "performance = pt.load_performance(performance_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "provincial-wagon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute piano roll\n",
    "use_piano_range = False\n",
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,\n",
    ")\n",
    "\n",
    "performance_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "420be1b4",
   "metadata": {},
   "source": [
    "Let's have a look at the output of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "minute-enemy",
   "metadata": {},
   "source": [
    "By default, piano rolls computed with partitura are stored in scipy's sparse matrices, since most of the elements are 0.\n",
    "\n",
    "The first dimension of the array is MIDI pitch (128) and the second dimension are discrete time-steps defined by the `time_div` and `time_unit` arguments of the  `compute_pianoroll` function.\n",
    "\n",
    "Let's visualize the piano rolls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 7))\n",
    "axes[0].imshow(\n",
    "    score_pr.todense(),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axes[1].imshow(\n",
    "    performance_pr.todense(),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "y_label = \"Piano key\" if use_piano_range else \"MIDI pitch\"\n",
    "axes[0].set_ylabel(y_label)\n",
    "axes[1].set_ylabel(y_label)\n",
    "axes[0].set_title(\"Score\")\n",
    "axes[1].set_title(\"Performance\")\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "atomic-addition",
   "metadata": {},
   "source": [
    "For more information, see the documentation of  [`compute_pianoroll`](https://partitura.readthedocs.io/en/latest/modules/partitura.utils.html#partitura.utils.compute_pianoroll)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "possible-oxford",
   "metadata": {},
   "source": [
    "### 2.2 Pitch Class Distributions\n",
    "\n",
    "These features are the symbolic equivalent to *chroma* features in audio. This representation is basically a piano roll that has been folded into a single octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "creative-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pc_pr = pt.utils.music.compute_pitch_class_pianoroll(\n",
    "    score,\n",
    "    normalize=True,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "urban-mediterranean",
   "metadata": {},
   "source": [
    "Let's plot this feature and compare it to a piano roll of the same score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4,\n",
    "    piano_range=False,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "axes[0].imshow(\n",
    "    score_pc_pr,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axes[0].set_title(\"Pitch Class Distribution\")\n",
    "axes[0].set_ylabel(\"Pitch classes\")\n",
    "axes[1].imshow(\n",
    "    score_pr.todense(),\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axes[1].set_title(\"Piano roll\")\n",
    "axes[1].set_ylabel(\"MIDI pitch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8d8d3c",
   "metadata": {},
   "source": [
    "## 3. Alignment Methods\n",
    "\n",
    "We move now to methods for computing the alignment between features from one version of a piece of music to another. Common methods are dynamic programming approaches like dynamic time warping (DTW) and probabilistic approaches like hidden Markov models.\n",
    "\n",
    "### 3.1 Alignments with Dynamic Time Warping.\n",
    "\n",
    "DTW is a dynamic programming algorithm to find the **optimal** alignment between to time-dependent sequences. In a nutshell, the DTW algorithm finds the alignment between two sequence in three steps:\n",
    "\n",
    "1. Compute the pairwise distance between elements in sequence $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "2. Compute the accumulated cost matrix\n",
    "3. Find the best alignment by backtracking \n",
    "\n",
    "We will explore these steps with a simple example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "collective-allergy",
   "metadata": {},
   "source": [
    "## 4 Music Alignment with DTW\n",
    "\n",
    "1. **Feature Computation**: Compute features from score and the performance (in this case the piano rolls)\n",
    "\n",
    "2. **Time-wise Alignment**: Compute the alignment between the sequences of features using DTW. This produces a time-wise alignment that tells us which time in the performance (in seconds) corresponds to which time in the score (in musical beats).\n",
    "\n",
    "3. **Note-level Alignment**: To get a note-level alignment, we match notes in the performance to notes in the score considering the time-wise alignment, and matching notes greedily by pitch.\n",
    "\n",
    "Dynamic Time Warping and related sequence alignment algorithms return a path between two sequences or time series. Note alignment of two polyphonic parts is categorically different from a time series alignment. To get to a note alignment, we need to figure out what notes are played at a specific time in the piano roll. Sometimes this information might be imprecise so we need to relax the search for notes at some piano roll time to find all relevant notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "double-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignment import fast_dynamic_time_warping\n",
    "\n",
    "score_pr, sidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pr, pidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=10,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pr.toarray().T\n",
    "performance_features = performance_pr.toarray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch_index, onset, offset, midi_pitch\n",
    "sidx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx correspond to notes in note_array\n",
    "snote_array = score.note_array()\n",
    "print(snote_array[:5])\n",
    "\n",
    "# Check that the pitch in the note array corresponds to\n",
    "# the fourth column in the indices from the note array\n",
    "assert all(snote_array[\"pitch\"] == sidx[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "demonstrated-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic time warping\n",
    "dtw_alignment = fast_dynamic_time_warping(\n",
    "    X=reference_features,\n",
    "    Y=performance_features,\n",
    "    metric=\"cityblock\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "buried-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_alignment,\n",
    "    idx1=sidx,\n",
    "    note_array1=score.note_array(),\n",
    "    idx2=pidx,\n",
    "    note_array2=performance.note_array(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_alignment[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9b2ac9e",
   "metadata": {},
   "source": [
    "### 4.1 Comparing alignments\n",
    "\n",
    "Let's compare different alignment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c33667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the ground truth alignment\n",
    "gt_alignment_fn = os.path.join(MATCH_DIR, \"Chopin_op10_no3_p01.match\")\n",
    "\n",
    "# Load the alignment and the performance\n",
    "performance, gt_alignment = pt.load_match(\n",
    "    gt_alignment_fn, pedal_threshold=127, first_note_at_zero=True\n",
    ")\n",
    "pnote_array = performance.note_array()\n",
    "\n",
    "# Load the score\n",
    "score_fn = os.path.join(MUSICXML_DIR, \"Chopin_op10_no3.musicxml\")\n",
    "score = pt.load_score(score_fn)\n",
    "snote_array = score.note_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "19f5cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features\n",
    "score_pcr, sidx = pt.utils.music.compute_pitch_class_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pcr, pidx = pt.utils.music.compute_pitch_class_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pcr.T\n",
    "performance_features = performance_pcr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b90b363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW\n",
    "dtw_pcr_warping_path = fast_dynamic_time_warping(\n",
    "    X=reference_features,\n",
    "    Y=performance_features,\n",
    "    metric=\"cityblock\",\n",
    ")\n",
    "\n",
    "dtw_pcr_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_pcr_warping_path,\n",
    "    idx1=sidx,\n",
    "    note_array1=snote_array,\n",
    "    idx2=pidx,\n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc97d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features\n",
    "score_pr, sidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pr, pidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pr.toarray().T\n",
    "performance_features = performance_pr.toarray().T\n",
    "\n",
    "# DTW\n",
    "dtw_pr_warping_path = fast_dynamic_time_warping(\n",
    "    X=reference_features,\n",
    "    Y=performance_features,\n",
    "    metric=\"cityblock\",\n",
    ")\n",
    "\n",
    "dtw_pr_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_pr_warping_path,\n",
    "    idx1=sidx,\n",
    "    note_array1=snote_array,\n",
    "    idx2=pidx,\n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "007b438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invent a linear alignment for testing\n",
    "from helper import dummy_linear_alignment\n",
    "\n",
    "# Dummy linear alignment\n",
    "linear_warping_path = dummy_linear_alignment(\n",
    "    X=reference_features,\n",
    "    Y=performance_features,\n",
    ")\n",
    "\n",
    "linear_alignment = greedy_note_alignment(\n",
    "    warping_path=linear_warping_path,\n",
    "    idx1=sidx,\n",
    "    note_array1=snote_array,\n",
    "    idx2=pidx,\n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].plot(\n",
    "    linear_warping_path[:, 0],\n",
    "    linear_warping_path[:, 1],\n",
    "    label=\"linear\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    dtw_pr_warping_path[:, 0],\n",
    "    dtw_pr_warping_path[:, 1],\n",
    "    label=\"DTW (piano roll)\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    dtw_pcr_warping_path[:, 0],\n",
    "    dtw_pcr_warping_path[:, 1],\n",
    "    label=\"DTW (pitch class)\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    linear_warping_path[:, 0],\n",
    "    linear_warping_path[:, 1],\n",
    "    label=\"linear\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    dtw_pr_warping_path[:, 0],\n",
    "    dtw_pr_warping_path[:, 1],\n",
    "    label=\"DTW (piano roll)\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    dtw_pcr_warping_path[:, 0],\n",
    "    dtw_pcr_warping_path[:, 1],\n",
    "    label=\"DTW (pitch class)\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Index in score\")\n",
    "axes[1].set_xlabel(\"Index in score\")\n",
    "axes[0].set_ylabel(\"Index in performance\")\n",
    "axes[1].set_xlim((200, 300))\n",
    "axes[1].set_ylim((450, 550))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wireless-projection",
   "metadata": {},
   "source": [
    "To inspect an alignment, we can use [**Parangonada**](https://sildater.github.io/parangonada/), a tool to compare alignments developed at our institute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "specific-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from partitura import save_parangonada_csv\n",
    "\n",
    "# Export files to Parangonada\n",
    "outdir = \"parangonada_files\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "save_parangonada_csv(\n",
    "    alignment=dtw_pr_alignment,\n",
    "    performance_data=performance,\n",
    "    score_data=score,\n",
    "    zalign=linear_alignment,\n",
    "    outdir=\"parangonada_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-integral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper import evaluate_alignment_notewise\n",
    "\n",
    "print(f\"Method\\tF-score\\tPrecision\\tRecall\")\n",
    "method = \"linear\"\n",
    "\n",
    "methods = [\n",
    "    (linear_alignment, \"linear\"),\n",
    "    (dtw_pr_alignment, \"DTW (piano roll)\"),\n",
    "    (dtw_pcr_alignment, \"DTW (pitch class)\"),\n",
    "]\n",
    "\n",
    "for align, method in methods:\n",
    "    precision, recall, fscore = evaluate_alignment_notewise(\n",
    "        prediction=align,\n",
    "        ground_truth=gt_alignment,\n",
    "    )\n",
    "    print(f\"{method}\\t{fscore:.4f}\\t{precision:.4f}\\t{recall:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "alike-doctor",
   "metadata": {},
   "source": [
    "## 5. Alignment Applications\n",
    "\n",
    "In this example, we are going to compare tempo curves of different performances of the same piece. Partitura includes a utility function called `get_time_maps_from_alignment`which creates functions (instances of Scipy's [interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)) that map score time to performance time (and the other way around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f48e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_tempo_curves\n",
    "\n",
    "# get all match files\n",
    "piece = \"Mozart_K331_1st-mov\"\n",
    "# piece = \"Schubert_D783_no15\"\n",
    "# piece = \"Chopin_op38\"\n",
    "# piece = \"Chopin_op10_no3\"\n",
    "\n",
    "plot_tempo_curves(piece, V4X22_DATASET_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
