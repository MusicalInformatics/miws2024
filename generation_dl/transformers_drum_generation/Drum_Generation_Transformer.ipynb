{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db255b2a",
   "metadata": {},
   "source": [
    "# Drum Generation Transformer\n",
    "\n",
    "In this tutorial, we will learn how to train a small auto-regressive transformer model for simplified drum beat generation in MIDI.\n",
    "\n",
    "At the end of this tutorial, we will be able to create a drum beat of a bar's length and investigate some useful techniques for encoding MIDI as input to a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011a3c8",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import partitura as pt\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a21c7d",
   "metadata": {},
   "source": [
    "### External files\n",
    "\n",
    "If run locally, external scripts are loaded directly from the cloned tutorial repository.\n",
    "\n",
    "If run in colab, external scripts are imported by httpimport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_helpers import (\n",
    "    INV_PITCH_DICT_SIMPLE,\n",
    "    tokens_2_notearray,\n",
    "    save_notearray_2_midifile,\n",
    "    generate_tokenized_data,\n",
    "    batch_data,\n",
    "    PositionalEncoding,\n",
    "    Transformer,\n",
    "    sample_from_logits,\n",
    "    sample_loop,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c723d9",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "I this tutorial we will work with the groove midi dataset, which contains MIDI drum grooves from a multitude of genres. For simplicity we load only the MIDI files in 4-4 from the dataset. For loading we use *Partitura*.\n",
    "If you want to load a precomputed and preprocessed dataset you can skip the next cells until the last cell before section 4 where it is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(os.getcwd(), \"./groove-v1.0.0-midionly\")\n",
    "\n",
    "typ_44 = [\n",
    "    os.path.basename(f)\n",
    "    for f in list(glob.glob(directory + \"/groove/*/*/*.mid\"))\n",
    "    if \"4-4\" in f\n",
    "]\n",
    "\n",
    "beat_type = [g.split(\"_\")[-2] for g in typ_44]\n",
    "tempo = [int(g.split(\"_\")[-3]) for g in typ_44]\n",
    "vals, counts = np.unique(tempo, return_counts=True)\n",
    "kde_tempo = gaussian_kde(tempo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a956c0d",
   "metadata": {},
   "source": [
    "### Histogram of tempos marked in file names\n",
    "\n",
    "Let's take a closer look at the data by plotting the histogram of tempi from the target pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    vals,\n",
    "    counts / counts.sum(),\n",
    "    width=4,\n",
    "    align=\"center\",\n",
    ")\n",
    "plt.plot(vals, kde_tempo(vals), c=\"firebrick\")\n",
    "plt.xlabel(\"Tempo (bpm)\")\n",
    "plt.ylabel(\"Relative freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a3e9b",
   "metadata": {},
   "source": [
    "We can almost notice a bi-modal distribution curve with a mean around a 100 beats per minute. Now let's define the `load_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab07ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    directory: str = \"./groove-v1.0.0-midionly\",\n",
    "    min_seq_length: int = 10,\n",
    "    time_sig: str = \"4-4\",\n",
    "    beat_type: Optional[str] = None,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    loads groove dataset data from directory\n",
    "    into a list of dictionaries containing the\n",
    "    note array as well as note sequence metadata.\n",
    "\n",
    "    Args:\n",
    "        directory: dir of the dataset\n",
    "        min_seq_length: minimal sequence length, a\n",
    "            all shorter performances are discarded\n",
    "        time_sig: only performances of this time sig\n",
    "            are loaded\n",
    "        beat_type: type of beat to load (\"beat\",\n",
    "            \"fill\", or None = both)\n",
    "\n",
    "    Returns:\n",
    "        a list of dicts containing note arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    files = glob.glob(directory + \"/groove/*/*/*.mid\")\n",
    "    files.sort()\n",
    "    sequences = []\n",
    "    if beat_type is None:\n",
    "        beat_types = [\"beat\", \"fill\"]\n",
    "    else:\n",
    "        beat_types = [beat_type]\n",
    "    for fn in files:\n",
    "        bn = os.path.basename(fn)\n",
    "        bns = bn.split(\"_\")\n",
    "        tempo = int(bns[-3])\n",
    "        bt = bns[-2]\n",
    "        ts = bns[-1].split(\".\")[-2]\n",
    "        if ts == time_sig and bt in beat_types:\n",
    "            seq = pt.load_performance_midi(fn)[0]\n",
    "            if len(seq.notes) > min_seq_length:\n",
    "                na = seq.note_array()\n",
    "                namax = (na[\"onset_tick\"] + na[\"duration_tick\"]).max()\n",
    "                namin = (na[\"onset_tick\"]).min()\n",
    "                dur_in_q = (namax - namin) / seq.ppq\n",
    "                seq_object = {\n",
    "                    \"id\": bn,\n",
    "                    \"na\": na,\n",
    "                    \"ppq\": seq.ppq,\n",
    "                    \"tempo\": tempo,\n",
    "                    \"beat_type\": bt,\n",
    "                    \"namax\": namax,\n",
    "                    \"namin\": namin,\n",
    "                    \"dur_in_q\": dur_in_q,\n",
    "                }\n",
    "                sequences.append(seq_object)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8aa2d5",
   "metadata": {},
   "source": [
    "#### Call the loading function (might take a moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2829ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = load_data(directory=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ac278",
   "metadata": {},
   "source": [
    "### Histogram of performance durations\n",
    "\n",
    "In this visualization example for the loaded data we plot the histogram of note durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_in_quarters = [k[\"dur_in_q\"] for k in seqs if k[\"beat_type\"] == \"beat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dur_in_quarters, bins=60)\n",
    "liq = np.array(dur_in_quarters)\n",
    "print((liq >= 4).sum(), (liq >= -4).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4994b15",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing: Segmentation & Tokenization\n",
    "\n",
    "**Reducing and Encoding the MIDI Sound Profiles**\n",
    "\n",
    "In this reduces drum generation we encode only 6 main sounds from a list of 22 standard perscussion MIDI sound slots.\n",
    "\n",
    "Here is a table that shows the Typical Sound to pitch association for drums in MIDI:\n",
    "\n",
    "\n",
    " **MIDI PITCH** | **STANDARD ASSOCIATED SOUND** | **REWRITEN SOUND**\n",
    "----------------|-------------------------------|--------------------\n",
    " 36             | Kick                          | Kick\n",
    " 37             | Snare (X-stick)               | Snare\n",
    " 38             | Snare (Head)                  | Snare\n",
    " 40             | Snare (Rim)                   | Snare\n",
    " 43             | Tom 1                         | Tom (Generic)\n",
    " 45             | Tom 2                         | Tom (Generic)\n",
    " 48             | Tom 3                         | Tom (Generic)\n",
    " 47             | Tom 2 (Rim)                   | Tom (Generic)\n",
    " 50             | Tom 1 (Rim)                   | Tom (Generic)\n",
    " 48             | Tom 3 (Rim)                   | Tom (Generic)\n",
    " 48             | Hi-Hat Closed (Bow)           | Hi-Hat\n",
    " 48             | Hi-Hat Closed (Edge)          | Hi-Hat\n",
    " 48             | Hi-Hat Open (Bow)             | Hi-Hat\n",
    " 48             | Hi-Hat Open (Edge)            | Hi-Hat\n",
    " 49             | Crash 1                       | Crash\n",
    " 55             | Crash 1                       | Crash\n",
    " 52             | Crash 2                       | Crash\n",
    " 57             | Crash 2                       | Crash\n",
    " 51             | Ride (bow)                    | Ride\n",
    " 59             | Ride (Edge)                   | Ride\n",
    " 53             | Ride (Bell)                   | Ride\n",
    " Any            | *Default*                     | *Default*\n",
    "\n",
    "In summary, we associate all the standard MIDI drum sounds to their sound *family*, i.e. Snare-type sounds to a Signle Snare sound, and so on.\n",
    "\n",
    "**Reducing the MIDI Velocity Encoding**\n",
    "\n",
    "For MIDI velocity values we encode 8 classes of velocity by applying:\n",
    "\n",
    "$$ newVel = oldVel / 2^4 $$\n",
    "\n",
    "We only accept integer values for $newVel$, so we are left with 8 classes of velocity.\n",
    "\n",
    "**Reducing the Tempo Encoding**\n",
    "\n",
    "Similarly, we reduce the different tempi found in the training data into 8 classses.\n",
    "\n",
    "**Beat of Fill Characterization**\n",
    "\n",
    "We also add information about each measure of the data being a beat or fill/solo part.\n",
    "\n",
    "**Reducing the Time Encoding**\n",
    "\n",
    "For the time encoding we use a hierarchical tree division in base 2.\n",
    "We start by separating the bar in 2, for first and second half note of the 4/4 bar.\n",
    "We continue for quarter, eight notes, and so on, up to 128 notes. So we use a binary setting for every hierarchical level.\n",
    "\n",
    "For example the binary hierarchical representation of the second 16th note of the bar would be :\n",
    "\n",
    "\n",
    "\n",
    " Half Note    |  Quarter Note   | 8-th Note | 16-th Note    |  32-th Note    | 64-th Note | 128-th Note\n",
    " -----------  | ---------       | --------- | ------------- | -------------- | ---------- | -----------\n",
    " 0   | 0 | 0 | 1 | 0 | 0 | 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_DICT_SIMPLE = {\n",
    "    36: 0,  # Kick\n",
    "    38: 1,  # Snare (Head)\n",
    "    40: 1,  # Snare (Rim)\n",
    "    37: 1,  # Snare X-Stick\n",
    "    48: 2,  # Tom 1\n",
    "    50: 2,  # Tom 1 (Rim)\n",
    "    45: 2,  # Tom 2\n",
    "    47: 2,  # Tom 2 (Rim)\n",
    "    43: 2,  # Tom 3\n",
    "    58: 2,  # Tom 3 (Rim)\n",
    "    46: 3,  # HH Open (Bow)\n",
    "    26: 3,  # HH Open (Edge)\n",
    "    42: 3,  # HH Closed (Bow)\n",
    "    22: 3,  # HH Closed (Edge)\n",
    "    44: 3,  # HH Pedal\n",
    "    49: 4,  # Crash 1\n",
    "    55: 4,  # Crash 1\n",
    "    57: 4,  # Crash 2\n",
    "    52: 4,  # Crash 2\n",
    "    51: 5,  # Ride (Bow)\n",
    "    59: 5,  # Ride (Edge)\n",
    "    53: 5,  # Ride (Bell)\n",
    "    \"default\": 6,\n",
    "}\n",
    "\n",
    "\n",
    "def pitch_encoder(pitch, pitch_dict=PITCH_DICT_SIMPLE):\n",
    "    # 22 different instruments  in dataset, default 23rd class\n",
    "    a = pitch_dict[\"default\"]\n",
    "    try:\n",
    "        a = pitch_dict[pitch]\n",
    "    except:\n",
    "        print(\"unknown instrument\")\n",
    "    return a\n",
    "\n",
    "\n",
    "def time_encoder(time_div, ppq):\n",
    "    # base 2 encoding of time, starting at half note, ending at 128th\n",
    "    power_two_classes = list()\n",
    "    for i in range(7):\n",
    "        power_two_classes.append(int(time_div // (ppq * (2 ** (1 - i)))))\n",
    "        time_div = time_div % (ppq * 2 ** (1 - i))\n",
    "    return power_two_classes\n",
    "\n",
    "\n",
    "def velocity_encoder(vel):\n",
    "    # 8 classes of velocity\n",
    "    return vel // 2**4\n",
    "\n",
    "\n",
    "def tempo_encoder(tmp):\n",
    "    # 8 classes of tempo between 60 and 180\n",
    "    return (np.clip(tmp, 60, 179) - 60) // 15\n",
    "\n",
    "\n",
    "def tokenizer(seq):\n",
    "    tokens = list()\n",
    "    na = seq[\"na\"]\n",
    "    fill_encoding = 0\n",
    "    if seq[\"beat_type\"] == \"fill\":\n",
    "        fill_encoding = 1\n",
    "    tempo_encoding = tempo_encoder(seq[\"tempo\"])\n",
    "\n",
    "    for note in na:\n",
    "        te = time_encoder(note[\"onset_tick\"] % (seq[\"ppq\"] * 4), seq[\"ppq\"])\n",
    "        pe = pitch_encoder(note[\"pitch\"])\n",
    "        ve = velocity_encoder(note[\"velocity\"])\n",
    "\n",
    "        tokens.append(te + [pe, ve, tempo_encoding, fill_encoding])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def measure_segmentation(seq, beats=4, minimal_notes=1):\n",
    "    mod = beats * seq[\"ppq\"]\n",
    "    no_of_measures = int(seq[\"namax\"] // mod + 1)\n",
    "\n",
    "    segmented_seq = list()\n",
    "    for measure_idx in range(no_of_measures):\n",
    "        na = seq[\"na\"]\n",
    "        new_na = np.copy(na[na[\"onset_tick\"] // mod == measure_idx])\n",
    "        if len(new_na) >= minimal_notes:\n",
    "            new_seq = copy.copy(seq)\n",
    "            new_seq[\"na\"] = new_na\n",
    "            segmented_seq.append(new_seq)\n",
    "        else:\n",
    "            continue\n",
    "    return segmented_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feff872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try uncommenting the next lines to see the outputs of these functions.\n",
    "# t = tokenizer(seqs[0])\n",
    "# ss = measure_segmentation(seqs[0], minimal_notes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First Tokenized Note of First sequence : {}\".format(t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90068bf0",
   "metadata": {},
   "source": [
    "### Training set generation (might take a moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f510c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"dataset129.pyc\"):\n",
    "\n",
    "    train_data = generate_tokenized_data(\n",
    "        seqs,\n",
    "        measure_segmentation,\n",
    "        tokenizer,\n",
    "        minimal_notes=20,\n",
    "    )\n",
    "    train_dataloader = batch_data(\n",
    "        data=train_data,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    with open(\"dataset129.pyc\", \"wb\") as fh:\n",
    "        pickle.dump(train_dataloader, fh)\n",
    "\n",
    "else:\n",
    "    with open(\"dataset129.pyc\", \"rb\") as fh:\n",
    "        train_dataloader = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try uncommenting the next lines to see the outputs of these functions.\n",
    "# train_dataloader[0].shape # batch, max sequence length, num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de36f1",
   "metadata": {},
   "source": [
    "## 4. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535b9ed",
   "metadata": {},
   "source": [
    "![MODEL](https://raw.githubusercontent.com/CPJKU/partitura_tutorial/mlflow/static/drumtransformer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d9a58",
   "metadata": {},
   "source": [
    "The embedding input and output dimension of our model uses the reduced representations that were described above.\n",
    "First, we input with the 7 levels binary hierarchical timing representation in one hot, next instrument (6 classes), velocity (6 classes), tempo (6 classes), and a binary beat as one hot. For the output, every one of those levels also contain a *START* or *END* token. Therefore, the binary one-hot representations go from 2 dimensional vectors to 4 dimensional where the third position is for the *START* token and the fourth position is for the *END* token.\n",
    "\n",
    "For consistency we repeated the same dimensions for the input Embeddings of the model. Of course this encodings are up to personal interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30334b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbedding(nn.Module):\n",
    "    \"\"\"A Multiembedding\"\"\"\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, tokens2dims: np.ndarray) -> None:\n",
    "        super().__init__()\n",
    "        # self.em_time0 = nn.Embedding(num_tokens, dim_model)\n",
    "        self.em_time0 = nn.Embedding(tokens2dims[0][0], tokens2dims[0][1])\n",
    "        self.em_time1 = nn.Embedding(tokens2dims[1][0], tokens2dims[1][1])\n",
    "        self.em_time2 = nn.Embedding(tokens2dims[2][0], tokens2dims[2][1])\n",
    "        self.em_time3 = nn.Embedding(tokens2dims[3][0], tokens2dims[3][1])\n",
    "        self.em_time4 = nn.Embedding(tokens2dims[4][0], tokens2dims[4][1])\n",
    "        self.em_time5 = nn.Embedding(tokens2dims[5][0], tokens2dims[5][1])\n",
    "        self.em_time6 = nn.Embedding(tokens2dims[6][0], tokens2dims[6][1])\n",
    "        self.em_pitch = nn.Embedding(tokens2dims[7][0], tokens2dims[7][1])\n",
    "        self.em_velocity = nn.Embedding(tokens2dims[8][0], tokens2dims[8][1])\n",
    "        self.em_tempo = nn.Embedding(tokens2dims[9][0], tokens2dims[9][1])\n",
    "        self.em_beat = nn.Embedding(tokens2dims[10][0], tokens2dims[10][1])\n",
    "        # self.total_dim = np.array([[2,2,2,2,2,2,2, # time encoding\n",
    "        #                  2,2,2,2]]).sum() # instrument, velocity, tempo, beat/fill\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Update Function and predict function of the model\"\"\"\n",
    "\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                self.em_time0(x[:, :, 0]),\n",
    "                self.em_time1(x[:, :, 1]),\n",
    "                self.em_time2(x[:, :, 2]),\n",
    "                self.em_time3(x[:, :, 3]),\n",
    "                self.em_time4(x[:, :, 4]),\n",
    "                self.em_time5(x[:, :, 5]),\n",
    "                self.em_time6(x[:, :, 6]),\n",
    "                self.em_pitch(x[:, :, 7]),\n",
    "                self.em_velocity(x[:, :, 8]),\n",
    "                self.em_tempo(x[:, :, 9]),\n",
    "                self.em_beat(x[:, :, 10]),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad68a5",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    opt: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    tokens2dims: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    \"\"\" \"\"\"\n",
    "    t2d = np.array(tokens2dims)\n",
    "    pred_dims = np.concatenate(([0], np.cumsum(t2d[:, 0])))\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        y = batch\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:, :-1, :]\n",
    "        y_expected = y[:, 1:, :]\n",
    "\n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again, that is batch / logits / sequence\n",
    "        pred = pred.permute(1, 2, 0)\n",
    "        loss = 0\n",
    "        for k in range(11):\n",
    "            # batch / logits / sequence ----- batch / sequence / tokens\n",
    "            loss += loss_fn(\n",
    "                pred[:, pred_dims[k] : pred_dims[k + 1], :], y_expected[:, :, k]\n",
    "            )\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: nn.Module,\n",
    "    opt: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    t2d: np.ndarray,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> List[float]:\n",
    "    \"\"\" \"\"\"\n",
    "    train_loss_list = []\n",
    "\n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\" * 25, f\"Epoch {epoch + 1}\", \"-\" * 25)\n",
    "\n",
    "        train_loss = train_loop(\n",
    "            model=model,\n",
    "            opt=opt,\n",
    "            loss_fn=loss_fn,\n",
    "            dataloader=train_dataloader,\n",
    "            tokens2dims=t2d,\n",
    "            device=device,\n",
    "        )\n",
    "        train_loss_list += [train_loss]\n",
    "\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print()\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b341c",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "\n",
    "The model figure above sets a clear direction for the input and output dimension of the model. So now we just need to initialize it likewise.\n",
    "The core model we use is a basic Transformer, that is already implemented in Pytorch. The Transformer model is not the focus of this tutorial so therefore we load it from a helper file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b844bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2dims = [\n",
    "    (4, 8),\n",
    "    (4, 8),\n",
    "    (4, 8),\n",
    "    (4, 4),\n",
    "    (4, 4),\n",
    "    (4, 4),\n",
    "    (4, 4),\n",
    "    (8, 16),\n",
    "    (10, 12),\n",
    "    (10, 12),\n",
    "    (4, 4),\n",
    "]\n",
    "\n",
    "model_spec = dict(\n",
    "    tokens2dims=tokens2dims,\n",
    "    num_heads=6,\n",
    "    num_decoder_layers=6,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    tokens2dims=model_spec[\"tokens2dims\"],\n",
    "    MultiEmbedding=MultiEmbedding,\n",
    "    num_heads=model_spec[\"num_heads\"],\n",
    "    num_decoder_layers=model_spec[\"num_decoder_layers\"],\n",
    "    dropout_p=model_spec[\"dropout_p\"],\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters in model: \", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e170c",
   "metadata": {},
   "source": [
    "## 6. LOOP (only call to retrain / finetrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cdce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"Drum_Transformer_500.pt\"\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "opt.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d54647",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "train_loss_list = fit(\n",
    "    model=model,\n",
    "    opt=opt,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    t2d=tokens2dims,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a96d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH = 500\n",
    "# PATH = f\"Drum_Transformer_{EPOCH}.pt\"\n",
    "# LOSS = train_loss_list[-1]\n",
    "# MODEL_SPEC = model_spec\n",
    "\n",
    "# torch.save(\n",
    "#     {\n",
    "#         \"epoch\": EPOCH,\n",
    "#         \"model_state_dict\": model.state_dict(),\n",
    "#         \"optimizer_state_dict\": opt.state_dict(),\n",
    "#         \"loss\": LOSS,\n",
    "#         \"model_spec\": MODEL_SPEC,\n",
    "#     },\n",
    "#     PATH,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817677b",
   "metadata": {},
   "source": [
    "## 7. SAMPLING\n",
    "\n",
    "To generate a drum beat we need to call sample_loop from the model.\n",
    "The sampling loop will start with a *START* token in all fields (Timing, tempo, etc.) and autoregressively predict one note at the time until having a pre-specified number of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad410c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_loop(model, tokens2dims, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    X.cpu().numpy()[0, 1:, :],\n",
    "    columns=[\n",
    "        \"Time Half Note\",\n",
    "        \"Time Quarter Note\",\n",
    "        \"Time 8th Note\",\n",
    "        \"Time 16th Note\",\n",
    "        \"Time 32nd Note\",\n",
    "        \"Time 64th Note\",\n",
    "        \"Time 128th Note\",\n",
    "        \"Pitch / Instrument\",\n",
    "        \"Velocity\",\n",
    "        \"Tempo\",\n",
    "        \"Beat type\",\n",
    "    ],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633c14a",
   "metadata": {},
   "source": [
    "We can see that the sample loop predicts 30 notes (apart from the *START* token) but only 16 actual MIDI notes where predicted and from line 16 and down we get only the *END* token.\n",
    "\n",
    "Finally, we can save generated MIDI files from the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(16):\n",
    "    XX = X.cpu().numpy()[k, 1:, :]\n",
    "    na = tokens_2_notearray(XX)\n",
    "    save_notearray_2_midifile(na, k, fn=\"GeneratedDrumBeats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371c54d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To play the MIDI files you can import them in your DAW of preference and load a drum VSTi or create your own DrumSampler.\n",
    "\n",
    "In this tutorial, we learned how to create MIDI drum generation using partitura and a Transformer autoregressive model.\n",
    "We investigated some possibilities for encoding and tokenization of timing, instrument, dynamics, and tempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
