{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Optimization with Genetic Algorithms\n",
    "\n",
    "This notebook is an introduction to Genetic Algorithms to minimize a real-valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Union, Callable\n",
    "\n",
    "RNG = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimize/maximize a function with genetic algorithms! In order to do so, we need to follow these steps:\n",
    "\n",
    "1. Generate a population of candidate solutions (randomly)\n",
    "2. Repeat until satisfied\n",
    "    1. Test the quality of the candidate solutions\n",
    "    2. Select solutions to reproduce\n",
    "    3. Produce new variations of selected solutions\n",
    "    4. Replace old solutions with new ones\n",
    "\n",
    "\n",
    "To illustrate this procedure, let's start with a very simple example. Consider the following function\n",
    "\n",
    "$$f(x) = (x - a)^2$$\n",
    "\n",
    "Let's define the function and see the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function that we want to minimize\n",
    "def objective_function(x: np.ndarray, a: float = 7.0) -> np.ndarray:\n",
    "    return (x - a) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the minimum of $f(x)$ is $a$. From elementary calculus, we know that $\\arg \\min_{x} f(x)$ is the solution of $\\frac{d f(x)}{dx} = 0$\n",
    "\n",
    "$$\\frac{d f(x)}{dx} = 2(x - a)$$\n",
    "\n",
    "$$2(x - a) = 0 \\Rightarrow x =  a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 7\n",
    "x = np.linspace(-2 * a, 2 * a)\n",
    "y = objective_function(x, a)\n",
    "plt.plot(x, y, c=\"navy\")\n",
    "\n",
    "plt.plot(\n",
    "    np.ones_like(x) * a,\n",
    "    np.linspace(y.min(), y.max(), len(x)),\n",
    "    c=\"firebrick\",\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm\n",
    "\n",
    "### Generating a population of solutions \n",
    "\n",
    "The first step for solving the problem is to generate a population of solutions. A common way to do this is to simply sample candidate solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Initial Population\n",
    "def initialize_population(pop_size: int, bounds: Tuple[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Initialize population drawing samples from a uniform distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pop_size : int\n",
    "        Size of the population\n",
    "    bounds: Tuple[int]\n",
    "        Lower and Upper bounds for the uniform distribution\n",
    "\n",
    "    Returns\n",
    "    np.ndarray\n",
    "        Initial population\n",
    "    \"\"\"\n",
    "    return RNG.uniform(bounds[0], bounds[1], pop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = initialize_population(50, (-2*a, 2*a))\n",
    "\n",
    "plt.scatter(population, objective_function(population))\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the quality of the solutions: Fitness function\n",
    "\n",
    "To evaluate the quality of the solutions (i.e., how well do they solve the problem), we define a simple fitness function that let us know how well the solutions minimize/maximize the problem\n",
    "\n",
    "Note that there are many possibilities here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness Evaluation\n",
    "def fitness_function(\n",
    "    x: np.ndarray,\n",
    "    objective_function: Callable[[np.ndarray], np.ndarray] = objective_function,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fitness function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input samples\n",
    "    \n",
    "    objective_function: Callable[[np.ndarray], np.ndarray]\n",
    "        Function that we want to optimize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Fitness function. Higher fitness for lower objective value\n",
    "    \"\"\"\n",
    "    return 1 / (1 + abs(objective_function(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, fitness_function(x))\n",
    "plt.scatter(\n",
    "    population,\n",
    "    fitness_function(population),\n",
    "    c=\"firebrick\",\n",
    "    alpha=0.75,\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"Fitness function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting solutions to reproduce\n",
    "\n",
    "There are several strategies to select solutions\n",
    "\n",
    "1. **Double Tournament Selection**:\n",
    "   - A group of $n$ solutions is randomly chosen, and the fittest one is selected as a parent.\n",
    "   - The process is repeated with a new group for the second parent.\n",
    "   - It can be done with or without replacement (allowing or disallowing the same parent to be picked twice).\n",
    "\n",
    "2. **Roulette Wheel Selection**:\n",
    "   - Parents are chosen with probabilities proportional to their fitness.\n",
    "   - A solutions's probability of selection is  $f_i/F$, where $f_i$ is its fitness and $F$ is the sum of fitness values in the population.\n",
    "\n",
    "3. **Rank Selection**:\n",
    "   - Similar to roulette selection but based on rank rather than raw fitness.\n",
    "   - Creatures are ranked by fitness, and probabilities are based on their rank values.\n",
    "   - The least fit creature has rank 1, giving it the lowest chance of being selected.\n",
    "\n",
    "   In here we implement the Roulette wheel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Function\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    output = np.exp(x - np.max(x))  # Stability trick to avoid overflow\n",
    "    return output / np.sum(output)\n",
    "\n",
    "\n",
    "# Selection (Roulette Wheel)\n",
    "def selection(pop: np.ndarray, fitness: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # We can use softmax instead of just \n",
    "    # the scaled fitness to ensure a normalized probability\n",
    "    # distribution\n",
    "    # prob = fitness / np.sum(fitness)\n",
    "    prob = softmax(fitness)\n",
    "    \n",
    "    chosen_idx = RNG.choice(len(pop), size=len(pop), p=prob)\n",
    "    return pop[chosen_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_solutions = selection(population, fitness_function(population))\n",
    "\n",
    "plt.scatter(\n",
    "    population,\n",
    "    fitness_function(population),\n",
    "    c=\"red\",\n",
    "    marker=\"+\",\n",
    ")\n",
    "plt.scatter(\n",
    "    selected_solutions,\n",
    "    fitness_function(selected_solutions),\n",
    "    c=\"green\",\n",
    "    marker=\"o\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing variations: Crossover and Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossover\n",
    "def crossover(parent1, parent2, crossover_rate=0.7):\n",
    "    if RNG.rand() < crossover_rate:\n",
    "        alpha = RNG.rand()  # Blend factor\n",
    "        child1 = alpha * parent1 + (1 - alpha) * parent2\n",
    "        child2 = alpha * parent2 + (1 - alpha) * parent1\n",
    "        return child1, child2\n",
    "    return parent1, parent2\n",
    "\n",
    "\n",
    "# Mutation\n",
    "def mutate(child, mutation_rate=0.1, bounds=(-10, 10)):\n",
    "    if RNG.rand() < mutation_rate:\n",
    "        mutation = RNG.uniform(bounds[0], bounds[1])\n",
    "        child += mutation\n",
    "    return child\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Let's put everything together into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Genetic Algorithm\n",
    "def genetic_algorithm(bounds, pop_size=500, generations=1000):\n",
    "    # Step 1: Initialize population\n",
    "    population = initialize_population(pop_size, bounds)\n",
    "    best_solutions = []\n",
    "\n",
    "    # Step 2: Repeat\n",
    "    for gen in range(generations):\n",
    "        # Step 2.1 Test quality of solutions\n",
    "        fitness = np.array([fitness_function(x) for x in population])\n",
    "        best_solutions.append(population[np.argmax(fitness)])\n",
    "\n",
    "        # Step 2.2 Selection\n",
    "        selected_pop = selection(population, fitness)\n",
    "\n",
    "        # Step 2.3 Create Next Generation\n",
    "        next_gen = []\n",
    "        for i in range(0, len(selected_pop), 2):\n",
    "            parent1 = selected_pop[i]\n",
    "            parent2 = selected_pop[min(i + 1, len(selected_pop) - 1)]\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            child1 = mutate(child1, bounds=bounds)\n",
    "            child2 = mutate(child2, bounds=bounds)\n",
    "            next_gen.extend([child1, child2])\n",
    "\n",
    "        population = np.clip(next_gen, bounds[0], bounds[1])\n",
    "\n",
    "    return population, best_solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Algorithm\n",
    "bounds = (-10, 10)\n",
    "pop_size = 50\n",
    "generations = 100\n",
    "\n",
    "final_population, best_solutions = genetic_algorithm(bounds, pop_size, generations)\n",
    "\n",
    "# Plot Results\n",
    "generations = np.arange(len(best_solutions))\n",
    "best_fitness = [objective_function(x) for x in best_solutions]\n",
    "\n",
    "plt.plot(generations, best_fitness, label=\"Best Solution\")\n",
    "plt.title(\"Convergence of Genetic Algorithm\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Objective Function Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best solution found:\", min(best_solutions))\n",
    "print(\"Function value at best solution:\", min(best_fitness))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miws24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
